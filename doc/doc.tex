% Compile with:
% latex --halt-on-error doc.tex && latex doc.tex && dvips doc.dvi && ps2pdf doc.ps

\documentclass{book}
\usepackage{amsmath}
% \usepackage{hyperref}
\usepackage{pstricks}

\input defs.tex

\title{Objective-First Nanophotonic Design Plan} 
\author{Jesse Lu}
\begin{document}
\maketitle
\tableofcontents


\chapter{Introduction}
\section{Problem statement}
Our goal is to create a software package 
    to enable the design of nanophotonic devices.
Specifically, our software produces designs for
    \emph{linear} nanophotonics devices based on
    \emph{desired coupling strengths} 
    between various input and output fields.

% Presently, almost all nanophotonic devices are designed
%     by guessing what a good design might look like,
%     based on intuition and experience,
%     and then optimizing it by trial-and-error.
% In contrast, we want to enable the \emph{inverse design}
%     of nanophotonic devices;
%     that is, to design devices by simply describing the 
%     desired performance that it must achieve.

To put things mathematically, we want to create software 
    to solve the following problem,
    \EE {the plan (single mode)}
        {\minimize&  f(x) + g(z) \\
        \subto&     A(z)x - b(z) = 0}
    where 
    \BI $x$ and $z$ are the variables representing 
            the \emph{field} our device produces and
            the \emph{structure} of the device respectively,
    \I  $f(x)$ and $g(z)$ are our \emph{design objectives},
            which tell us the desirable properties 
            that we would like our device to achieve, and
    \I  $A(z)x - b(z)$ is the \emph{physics residual},
            the underlying physical laws which must be met. \EI

In general, we need to consider multiple fields produced 
    by the device.
Our problem statement is then
    \EE {the plan (multi-mode)}
        {\minimize&  \sum_i^N f_i(x_i) + g(z) \\
        \subto&     A_i(z)x_i - b_i(z) = 0,\quad\text{for $i = 1, \ldots, N.$}}

\section{Structure of our software}
Our software package consists of various modules
    which are designed to be maximally orthogonal.
The various modules are illustrated in \fig{structure}.

\begin{figure}[ht]\begin{center}
\begin{pspicture}(6,2.4)(-6,-2)
\psset{gridcolor=green, subgridcolor=yellow}
    \let\psgrid\relax

    \rput[t](-1.5,1){physics}
    \psframe(-0.2,0.4)(-2.8,1.4)

    \rput[t](1.5,1){simulation}
    \psframe(0.2,0.4)(2.8,1.4)

    \psline[linestyle=dashed](-3.2,0.2)(3.2,0.2)
    \rput[l](3.2,0.2){\parbox{3cm}{\center physics-to-algebra \\ translation layer}}
    \rput[t](0,-0.4){optimization}
    \psframe(-1.3,-1)(1.3,0)
\end{pspicture}
\caption{Structure of our software. 
    The physics and simulation modules provide a complete description of the problem.
    The translation module takes this description and casts it in the language of linear algebra,
        which the optimization module can then use to produce various designs.}
\label{fig:structure}
\end{center} \end{figure}

Although all modules are provided by our software, 
    the majority of the innovation lies in the optimization module.
Since the optimization module works entirely in the realm of linear algebra,
    a translation module is provided in order to cast concepts from electromagnetics into this form.
Finally, physics and simulation modules are provided 
    to allow the user to describe the design problem in physical, rather than mathematical terms.
    


\chapter{Theory overview}
\section{Mathematically rigorous statement of the problem}
As previously stated, the problem we want to solve is 
    \EE {rigorous problem statement}
        {\minimize&  \sum_i^N f_i(x_i) + g(z) \\
        \subto&     A_i(z)x_i - b_i(z) = 0,\quad\text{for $i = 1, \ldots, N.$}}
To now be more precise,
    \BI $x_i \in \comps^m$ are the field variables,
    \I  $z \in \comps^n$ is the structure variable,
    \I  $f_i(x_i) \in \comps^m \to \reals$ are the field design objectives,
    \I  $g(z) \in \comps^n \to \reals$ is the structure design objective,
    \I  $A_i(z)x_i - b_i(z)$ are the physics residuals, with
    \I  $A_i(z) \in \comps^{n \times n}$ and
    \I  $b_i(z) \in \comps^n$. \EI

\section{Definition of physics residual}
The physics residual, $A_i(z)x_i - b_i(z)$, 
    corresponds to the electromagnetic wave equation
    \E  {wave equation in E}
        {(\curl \mu^{-1} \curl - \omega^2 \epsilon) E = -i \omega J}
        via
    \BI $\curl \mu^{-1} \curl - \omega^2 \epsilon \to A_i(z)$, % where $\epsilon$ is a function of $z$,
    \I  $\epsilon \to S_i z + \epsilon_{\text{const}, i}$,
    \I  $E \to x_i$, and
    \I  $-i \omega J \to b_i(z)$, typically constant with respect to $z$. \EI

\section{Bi-affine property of the physics residual}
Critically, \eq{wave equation in E} is not only linear in $E$,
    but is also affine in $\epsilon$.
This allows us to form the extremely useful relationship,
    \E  {bi-affine property}
        {A_i(z)x_i - b_i(z) = B_i(x_i)z - d_i(x_i) = 0,}
    where $B(x_i) \in \comps^{m \times n}$, $d(x_i) \in \comps^n$ and
    \BI  $-\omega^2 E \to B_i(x_i)$, 
    \I  $\curl \mu^{-1} \curl E \to d_i(x_i)$. \EI

\section{Definition of the field design objective}
Although the field design objective $f_i(x_i)$ 
    can take on virtually any form,
    we choose to define it very specifically as
    \E  {field design objective definition}
        {f_i(x_i) = \sum_j I_+(|c_{ij}\T x_i| - \alpha_{ij})
            + I_+(\beta_{ij} - |c_{ij}\T x_i|),}
    where $c_{ij} \in \comps^m$ and 
    $I_+$ is the indicator function on nonnegative reals,
    \E{}{I_+(u) = \begin{cases} 0 & u \ge 0, \\ 
                                \infty & u < 0. \end{cases}}

Such a design objective implements the constraints 
    $\alpha_{ij} \le |c_{ij}\T x_i| \le \beta_{ij}$
    which can be interpreted physically as 
    constraining the power emitted into the optical modes
    represented by $c_{ij}$.

\section{Choice of the structure design objective}
The form of the structure design objective consists of two parts,
    \BI a structure parameterization function 
        $m(p) \in \reals^l \to \comps^n$, and 
    \I  a parameter weighting function $w(p)\in \reals^l \to \reals$. \EI
Both use the variable $p$ which allows us 
    to cast $z$ into arbitrary parameters of our choice.
Together, the general form of the structure design objective is
    \E  {structure design objective definition}
        {g(z) = I_0(z - m(p)) + w(p),}
    where $I_0$ is the indicator function on the set containing only $0$,
    \E{}{I_0(u) = \begin{cases} 0 & u = 0, \\ 
                                \infty & \text{otherwise.} \end{cases}}
An alternative interpretation of \eq{structure design objective definition} 
    is that it implements a hard constraint on $z$ 
    via the parameterization function $m(z)$,
    as well as a soft constraint on $z$ 
    via the weighting function $w(z)$.

Additionally, constraints on the elements of $p$ include either
    a bounded continuous range of allowable values, or
    a finite set of discrete allowable values.

% \section{Properties of the problem}
% We now examine the general mathematical properties 
%     of the stated problem \eq{rigorous problem statement}.

\section{Convexity analysis}
% \subsection{Joint non-convexity}
First we note that, as presented, 
    \eq{rigorous problem statement} is non-convex 
    in the variables $x_i$ and $z$. % Ref boyd book
Not only are the physics residuals $A_i(z)x_i - b_i(z)$ non-convex,
    but the field design objective, in that it implements the 
    $ \alpha_{ij} \le |c_{ij}\T x_i| $ constraint, is non-convex as well.

That our problem is non-convex means that it is fundamentally hard to solve
    because of the existence of multiple local minima.
Additionally, even if we were to arrive at the global maxima,
    we would not have a straightforward way to verify global optimality.
Lastly, fast convergence even to local minima may be difficult
    because methods such as Newton's method
    can not be directly applied to non-convex problems.

% \subsection{Separable convexity}
That said, \eq{rigorous problem statement} is \emph{separably convex} 
    in $x_i$ and $z$ if we ignore the non-convexity
    in the field design objectives, $f_i(x_i)$, and
    assume that the structure design objective, $g(z)$, is convex as well.
This is given because of the bi-affine property
    of the physics residual, as shown in \eq{bi-affine property}.

The separably convex, or \emph{bi-convex}, properties of our problem
    open the door for the use of alternating direction algorithms,
    and specifically the alternating directions 
    methods of multipliers (ADMM), % Ref. 
    for the implementation of a ``global'' optimization paradigm,
Of course, in this context we do not use the term ``global'' rigorously,
    but only to differentiate it from strategies that rely
    purely on local information.

Lastly, since our problem is not bi-convex 
    in the case of non-convex field or structure design objectives,
    simple extensions to alternating direction algorithms are employed.

% \section{Optimality condition}
% Based on the problem definition in \eq{rigorous problem statement},
%     we can write down a set of equations that, when met,
%     signal that we have arrived at a locally optimal point.
% Otherwise known as the Karush-Kuhn-Tucker (KKT) conditions, % Ref boyd
%     these are, for \eq{rigorous problem statement},
%     \EE {KKT conditions} % Check how these work out for complex values!!!
%         {|c_{ij}\T x_i| - \beta_{ij} &\le 0, \\
%         \alpha_{ij} - |c_{ij}\T x_i| &\le 0, \\
%         A_i(z)x_i - b_i(z) &= 0, \\
% %         % Lambda dual variables not needed because they do not appear 
% %         % in the last KKT condition.
% %         \lambda_{\alpha ij} &\ge 0, \\ 
% %         \lambda_{\beta ij} &\ge 0, \\
% %         \lambda_{\alpha ij}(|c_{ij}\T x_i| - \beta_{ij}) &= 0, \\
% %         \lambda_{\beta ij}(\alpha_{ij} - |c_{ij}\T x_i|) &= 0, \\
%         \nabla_z g(z) + \sum_i B(x_i)\T \nu_i &= 0,
%         }
%     where $\nu_i \in \comps^n$ are dual variables.
% 

\chapter{The optimization module}

\section{Capabilities of the module}
We have implemented various 
    \emph{optimization paradigms} and \emph{structure updates}
    which a user can arbitrarily combine in order to 
    to solve \eq{the plan (multi-mode)}.

Specifically, the user can choose to work in either
    a \emph{local} or \emph{global} optimization paradigm:
    \BI the local paradigm uses the adjoint method
            to find small changes in the structure which
            will decrease the design objective;
    \I  the global paradigm uses the objective-first method
            to arrive at a structure by forcing
            the design objective to be met from the start. \EI

In addition, the user can choose between \emph{continuous} or \emph{discrete}
    structure updates:
    \BI a continuous update constrains the values of $p$
        to be within a continous, bounded range; on the other hand,
    \I  a discrete update constrains the values of $p$ 
        within a finite set of allowable values. \EI

\section{Structure of the module}

The module is naturally separated into two submodules, 
    the optimization paradigm (or simply, paradigm) submodule 
    and the structure parameterization (or simply, structure) submodule.
The interaction between these two submodules is illustrated in \fig{strategy}.

\begin{figure}[ht]\begin{center}
\begin{pspicture}(6,5)(-6,-2)
\psset{gridcolor=green, subgridcolor=yellow}
    \let\psgrid\relax

    % Paradigm
    \rput(0,4.3){optimization paradigm}
    \psline(-2.2,4)(2.2,4) % top
    \psline(-2.2,4)(-2.2,2) % left
    \psline(2.2,4)(2.2,2) % right
    \psline(-2.2,2)(-1,2)
    \psline(2.2,2)(1,2)
    \psline(-1,2.2)(1,2.2)
    \psline(-1,2)(-1,2.2)
    \psline(1,2)(1,2.2)

    % Parameterization
    \psframe(-.8,0.8)(.8,-0.4) 
    % \rput[t](0,-0.6){\parbox{3cm}{\center structure\\ update}}
    \rput[t](0,-0.6){structure update}

    % Arrows
    \rput[r](-4.3,3){$z_\text{init}$}
    \psline{->}(-4,3)(-2.4,3)

    \rput[r](-1.9,0.8){$Q(z)$}
    \psline(-1.6,1.8)(-1.6,0.2)
    \psline{->}(-1.6,0.2)(-1,0.2)

    \rput[l](1.9,0.8){$\argmin Q(z) + g(z)$}
    \psline(1.6,0.2)(1,0.2)
    \psline{<-}(1.6,1.8)(1.6,0.2)


    \rput[l](4.3,3){$z_\text{final}$}
    \psline{<-}(4,3)(2.4,3)
\end{pspicture}
\caption{Basic layout of the module.
        The optimization paradigm submodule
            accepts an initial structure $z_\text{init}$
            and returns a final, optimized structure $z_\text{final}$.
        This is accomplished by repeatedly passing quadratic functions $Q(z)$ 
            to the structure update submodule,
            which returns an updated structure $z$
            which minimizes $Q(z) + g(z)$.}
\label{fig:strategy}
\end{center} \end{figure}

In essense, a design is achieved via repeated calls to the structure submodule
    by the paradigm submodule,
    in which carefully chosen quadratic functions $Q(z)$ are minimized.

\section{The paradigm submodule}
\subsection{The local optimization paradigm}
The local optimization paradigm applies the adjoint method to solve \eq{rigorous problem statement}
    in a manner analogous to the steepest-descent strategy.
Specifically, we compute the derivative of the total field design objective with respect to $z$,
    \E{total derivative of field design objective}
        {\grad_z F = \sum_i^N \grad_z f_i(x_i),}
    and then pass 
    \E{}{Q(z) = \frac{1}{2}\norm{z-z_0}^2 + \kappa \grad_z F^\cT (z-z_0)}
    to the structure submodule, for some $\kappa$.

The analogy with a steepest-descent strategy is apparent since 
    the global minimum of $Q(z)$ can be found via
    \E{}{\grad_z Q(z) = (z - z_0) + \kappa \grad F = 0,}
    resulting in 
    \E{}{z = z_0 - \kappa \grad_z F,}
    from which we see that the role of $\kappa$ is to determine the step-size 
    in the direction of steepest-descent.

\subsubsection{Computation of $\grad_z F$}
The derivatives of the individual field design objectives are found via
    \E{}{\frac{d}{dz} f_i(x_i) = \frac{\pd f_i}{\pd x_i} \frac{dx_i}{dz}.}
To obtain ${dx_i}/{dz}$ we first differentiate the corresponding physics residual, 
    $A_i(z) x_i - b_i(z)$,
    \E{}{A_i(z) dx_i + dA_i(z) - db_i(z) = A_i(z)dx_i + B_i(x_i)dz,}
    where $dA_i(z) - db_i(z) = B_i(x_i)dz$ 
    as a result of the bi-affine property of the physics residual \eq{bi-affine property}.

Assuming that physics is already satisfied, $A_i(z) x_i - b_i(z) = 0$,
    and that we want to keep the physics residual at 0,
    the following condition must then be satisfied,
    \E{}{A_i(z) dx_i = -B_i(x_i) dz}
    from which we obtain
    \E{}{\frac{dx}{dz} = -A_i(z)^{-1} B_i(x_i).}

Efficient calculation of $dx_i/dz$ is almost always impossible since $B_i(x_i) \in \comps^{n \times n}$.
At the same time, since ${\pd f_i}/{\pd x_i} \in \comps^{1 \times m}$, we instead compute $df_i(x_i)/dz$ via
    \E{}{\frac{d}{dz} f_i(x_i) = -\frac{\pd f_i}{\pd x_i} A_i(z)^{-1} B_i(x_i) = 
        -\left(A_i(z)^{-\cT} \frac{\pd f_i}{\pd x_i}^\cT\right)^\cT B_i(x_i)}
    which requires only one solve of $A_i^\cT$ as opposed to $n$ solves of $A_i$.

\subsubsection{Computation of $\pd f_i(x_i) / \pd x_i$}
The definition of $f_i(x_i)$ as given in \eq{field design objective definition} is not differentiable 
    since any deviation away from the power constraints results in $f_i = \infty$.
In order to make the constraints differentiable, then, 
    we use the following relaxed indicator function $I^\text{rel}_+$ in place of $I_+$,
    \E{}{I^\text{rel}_+(u) = \begin{cases} 0 & u \ge 0, \\ 
                                \frac{1}{a}|u|^p & u < 0, \end{cases}}
    where $a$ is a normalization factor and $p \in (0, \infty]$. 
In order to guarantee a well-defined value of $\pd f_i(x_i) / \pd x_i$ even for $p = \infty$,
    we can let $a = \max_i f_i(x_i)$.



\subsection{The global optimization paradigm}
In contrast to the local paradigm, 
    the global optimization paradigm utilizes an objective-first strategy
    in the design process.
Although such a strategy is not technically global,
    it is differentiated from local strategies because
    it strictly enforces all design objectives,
    even at the expense of non-zero physics residuals.

The optimization paradigm utilizes 
    the alternating directions method of multipliers\cite{ADMM} (ADMM) optimization technique, 
    which casts \eq{rigorous problem statement} in terms of the following augmented Lagrangians, 
    \E{}{\Lag(x, z, y) = \sum_i^N f_i(x_i) 
            + \frac{\rho}{2} \norm{A_i(z) x_i - b_i(z)}^2 
            +\real\left[y_i^\cT (A_i(z)x_i-b_i(z))\right] + g(z),}
    where $\rho$ is a constant scalar and 
    $y_i$ are dual variables. If we introduce $u_i = y_i / \rho$, then 
    \E{}{\Lag(x,z,u) = \sum_i^N f_i(x_i)
            + \frac{\rho}{2} \norm{A_i(z) x_i - b_i(z) + u_i}^2 
            - \frac{\rho}{2} \norm{u_i}^2 + g(z).}

ADMM obtains the solutions for $x$ and $z$ via
    \EE{}{  x_i &\gets \argmin_{x_i} \Lag(x, z, u), \quad i = 1,\ldots,N \\
            z &\gets \argmin_{z} \Lag(x, z, u), \\
            u_i &\gets u_i + (A_i(x_i) z - b(x_i)), \quad i = 1,\ldots,N.}

To obtain the correct $Q(z)$ which should be passed to the structure submodule,
    we use \eq{bi-affine property} to write
    \E{}{\argmin_z \Lag = \argmin_z g(z) 
            + \sum_i^N \frac{\rho}{2} \norm{B_i(x_i) z - d(x_i) + u_i}^2}
    which means that
    \E{}{Q(z) = \sum_i^N \frac{\rho}{2} \norm{B_i(x_i) z - d(x_i) + u_i}^2}

\subsubsection{Computation of $\argmin_{x_i} \Lag$}
Updating $x_i$, which is left to the paradigm submodule,
    involves solving $\argmin_{x_i} \Lag(x, z, u)$ via Newton's method for all $i = 1, \ldots, N$.
This can be accomplished via the gradient with respect to $x_i$
    \E{}{\grad_{x_i}\Lag(x, z, u) = \grad_{x_i} f_i(x_i)
                + \rho A_i(z)\T (A_i(z) x_i - b_i(z) + u_i),}
    as well as the Hessian with respect to $x_i$
    \E{}{\grad^2_{x_i}\Lag(x, z, u) = \grad^2_{x_i} f_{x_i} + \rho A_i(z)\T A_i(z),}
    which can be combined to determine the direction of a step in Newton's method,
    \E{}{\Delta x_i = - \grad^2_{x_i} \Lag^{-1} \grad_{x_i} \Lag.}

Solving $\argmin_{x_i} \Lag(x, z, u)$ from an initial $x_i$ thus proceeds by
    repeatedly computing $\Delta x_i$ and then performing a line search along that direction,
    until some convergence criterion is met, typically
    \E{}{\frac{1}{2} \grad_{x_i}\Lag\T \grad^2_{x_i} \Lag^{-1} \grad_{x_i} \Lag \le \text{error threshold}.}

\subsubsection{Relaxation of $f_i(x_i)$}
In order to compute $\Delta x_i$ we relax the design objectives, $f_i(x_i)$, to a differentiable, and convex form.
Specifically, the upper bound constraint is relaxed via
    \E{}{I_+(\beta_{ij} - |c\T_{ij} x_i|) \to -\frac{1}{t} \ln (\beta_{ij}^2 - \norm{c\T_{ij} x_i}^2),}
    where $t$ is a real positive scalar. The lower bound constraint is relaxed via
    \E{}{I_+(|c\T_{ij} x_i| - \alpha_{ij}) \to -\frac{1}{t} \ln (\real[e^{-i\phi_{ij}}c\T_{ij} x_i] - \alpha_{ij}),}
     where $e^{-i\phi_{ij}}$ is a phase factor on $c\T_{ij} x_i$.
The convention of $\phi_{ij} = \text{angle}(c\T_{ij} x_i)$ is often used
    to determine $\phi_{ij}$ before optimizing for $x_i$.

The gradient can now be obtained as
    \E{}{\grad_{x_i} f_i(x_i) = \sum_j \frac{r_{ij}}{t} c_{ij}}
    where
    \E{}{r_{ij} = \frac{2c\T_{ij} x_i}{\beta_{ij}^2 - \norm{c\T_{ij} x_i}^2} 
                - \frac{e^{i\phi_{ij}}}{\real[e^{-i\phi_{ij}} c\T_{ij} x_i] - \alpha_{ij}},}
    and the Hessian as 
    \E{}{\grad_{x_i}^2 f_i(x_i) = \sum_j \frac{s_{ij}}{t} c_{ij} c\T_{ij}}
    where
    \E{}{s_{ij} = \frac{2}{\beta_{ij}^2 - \norm{c\T_{ij} x_i}^2}
                + \frac{4\norm{c\T_{ij} x_i}^2}{(\beta_{ij}^2 - \norm{c\T_{ij} x_i}^2)^2}
                + \frac{1}{(\real[e^{-i\phi_{ij}} c\T_{ij} x_i] - \alpha_{ij})^2}.}



\subsubsection{Computation of $\Delta x_i$}
We now show how $\Delta x_i$ may be efficiently computed.
First we introduce
    \E{}{\tilde{c}_i = \frac{1}{\rho t} \sum_j r_j A^{-\cT} c_{ij}}
    in order to write
    \E{}{\grad_{x_i} \Lag(x, z, u) = \rho A_i(z)\T(A_i(z)x_i - b_i(z) 
                            + u_i + \tilde{c}_i).}
For its part, the Hessian can be simplified using the matrix inversion lemma\footnote{$(A + UCV)^{-1} = A^{-1} - A^{-1}U (C^{-1} + V A^{-1} U)^{-1} V A^{-1}$}.
Coupled with the introduction of 
    \E{}{\tilde{C}_i = A_i(z)^{-\cT}\begin{bmatrix} 
                        \sqrt{\frac{s_1}{\rho t}}c_{i1} &
                        \sqrt{\frac{s_2}{\rho t}}c_{i1} &
                        \cdots &
                        \sqrt{\frac{s_{m_i}}{\rho t}}c_{im_i} 
                        \end{bmatrix}}
    we can write
    \E{}{\grad_{x_i}^2 \Lag(x, z, u) = \rho A_i(z)\T (I + \tilde{C}\tilde{C}\T) A_i(z),}
    and more importantly
    \E{}{(\grad_{x_i}^2 \Lag(x, z, u))^{-1} = \frac{1}{\rho} A_i(z)^{-1} M A_i(z)^{-\cT},}
    where
    \E{}{M = (I + \tilde{C} \tilde{C}\T)^{-1} = 
            I - \tilde{C} (I + \tilde{C}\T\tilde{C})^{-1} \tilde{C}\T.}
 
Finally, the expression for $\Delta x_i$ is obtained as
    \E{}{\Delta x_i = - \grad^2_{x_i} \Lag^{-1} \grad_{x_i} \Lag = 
        A_i(z)^{-1} M (A_i(z)x_i - b_i(z) + u_i + \tilde{c}_i).}
        
    
\section{The structure submodule}
\subsection{The continuous structure update}
A continuous structure update scheme solves
    \E  {structure submodule primary equation}
        {\argmin Q(z) + g(z)}
    by limiting the allowable values of $p$
    in \eq{structure design objective definition} to a continuous range
    of real values.

If both $m(p)$ and $w(p)$ are linear, 
    then \eq{structure submodule primary equation} is convex
    and can be solved using CVX.

On the other hand, if this is not the case, 
    then \eq{structure submodule primary equation}
    is solved using a gradient-descent method.


\subsection{The discrete structure update}
A discrete structure update scheme solves 
    \eq{structure submodule primary equation}
    while limiting the range of $p$ to a finite set of discrete values.

In the special case where both $m(p)$ and $w(p)$, as well as $Q(z)$
    are all linear and diagonal,
    then \eq{structure submodule primary equation} may be solved trivially.
Naturally, this is a very unique case and requires some amount 
    of care to set up on the user's part.

In general, \eq{structure submodule primary equation} is solved 
    using a greedy algorithm where the most beneficial change
    in a single element of $p$ is taken until no beneficial steps remain.



\begin{thebibliography}{99}
\bibitem{ADMM} Boyd group ADMM paper
\end{thebibliography}
\end{document}
