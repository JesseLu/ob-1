% Use the following command to compile:
% latex --halt-on-error doc.tex && latex doc.tex && dvips doc.dvi && ps2pdf doc.ps

\documentclass{book}
\usepackage{amsmath}
% \usepackage{hyperref}
\usepackage{pstricks}

\input defs.tex

\title{Objective-First Nanophotonic Design Plan} 
\author{Jesse Lu}
\begin{document}
\maketitle
\tableofcontents


\chapter{Introduction}
\section{Problem statement}
Our goal is to create a software package 
    to enable the design of nanophotonic devices.
Specifically, our software produces designs for
    \emph{linear} nanophotonics devices based on
    \emph{desired coupling strengths} between various optical ports.

% Presently, almost all nanophotonic devices are designed
%     by guessing what a good design might look like,
%     based on intuition and experience,
%     and then optimizing it by trial-and-error.
% In contrast, we want to enable the \emph{inverse design}
%     of nanophotonic devices;
%     that is, to design devices by simply describing the 
%     desired performance that it must achieve.

To put things mathematically, we want to create software 
    to solve the following problem,
    \EE {the plan (single mode)}
        {\minimize&  f(x) + g(z) \\
        \subto&     A(z)x - b(z) = 0}
    where 
    \BI $x$ and $z$ are the variables representing 
            the \emph{field} our device produces and
            the \emph{structure} of the device respectively,
    \I  $f(x)$ and $g(z)$ are our \emph{design objectives},
            which tell us the desirable properties 
            that we would like our device to achieve, and
    \I  $A(z)x - b(z)$ is the \emph{physics residual},
            the underlying physical laws which must be met. \EI

In general, we need to consider multiple fields produced 
    by the device.
Our problem statement is then
    \EE {the plan (multi-mode)}
        {\minimize&  \sum_i^N f_i(x_i) + g(z) \\
        \subto&     A_i(z)x_i - b_i(z) = 0,\quad\text{for $i = 1, \ldots, N.$}}

\section{Structure of our software}
Our software package consists of various modules
    which are designed to be maximally orthogonal.
The various modules are illustrated in \fig{structure}.

\begin{figure}[ht]\begin{center}
\begin{pspicture}(6,2.4)(-6,-2)
\psset{gridcolor=green, subgridcolor=yellow}
    \let\psgrid\relax

    \rput[t](-1.5,1){physics}
    \psframe(-0.2,0.4)(-2.8,1.4)

    \rput[t](1.5,1){simulation}
    \psframe(0.2,0.4)(2.8,1.4)

    \psline[linestyle=dashed](-3.2,0.2)(3.2,0.2)
    \rput[l](3.2,0.2){\parbox{3cm}{\center physics-to-algebra \\ translation layer}}
    \rput[t](0,-0.4){optimization}
    \psframe(-1.3,-1)(1.3,0)
\end{pspicture}
\caption{Structure of our software. 
    The physics and simulation modules provide a complete description of the problem.
    The translation module takes this description and casts it in the language of linear algebra,
        which the optimization module can then use to produce various designs.}
\label{fig:structure}
\end{center} \end{figure}

Although all modules are provided by our software, 
    the majority of the innovation lies in the optimization module.
Since the optimization module works entirely in the realm of linear algebra,
    a translation module is provided in order to cast concepts from electromagnetics into this form.
Finally, physics and simulation modules are provided 
    to allow the user to describe the design problem in physical, rather than mathematical terms.
    


\chapter{Theory}
\section{Mathematically rigorous statement of the problem}
As previously stated, the problem we want to solve is 
    \EE {rigorous problem statement}
        {\minimize&  \sum_i^N f_i(x_i) + g(z) \\
        \subto&     A_i(z)x_i - b_i(z) = 0,\quad\text{for $i = 1, \ldots, N.$}}
To now be more precise,
    \BI $x_i \in \comps^n$ are the field variables,
    \I  $z \in \reals^n$ is the structure variable,
    \I  $f_i(x_i) \in \comps^n \to \reals$ are the field design objectives,
    \I  $g(z) \in \reals^n \to \reals$ is the structure design objective,
    \I  $A_i(z)x_i - b_i(z)$ are the physics residuals, with
    \I  $A_i(z) \in \comps^{n \times n}$ and
    \I  $b_i(z) \in \comps^n$. \EI

\subsection{Definition of physics residual}
The physics residual, $A_i(z)x_i - b_i(z)$, 
    corresponds to the electromagnetic wave equation
    \E  {wave equation in E}
        {(\curl \mu^{-1} \curl - \omega^2 \epsilon) E = -i \omega J}
        via
    \BI $\curl \mu^{-1} \curl - \omega^2 \epsilon \to A_i(z)$, where $\epsilon$ is a function of $z$,
    \I  $E \to x_i$, and
    \I  $-i \omega J \to b_i(z)$, typically constant with respect to $z$. \EI

\subsection{Bi-affine property of the physics residual}
Critically, \eq{wave equation in E} is not only linear in $E$,
    but is also affine in $\epsilon$.
This allows us to form the extremely useful relationship,
    \E  {bi-affine property}
        {A_i(z)x_i - b_i(z) = B_i(x_i)z - d_i(x_i) = 0,}
    where $B(x_i) \in \comps^{n \times n}$, $d(x_i) \in \comps^n$ and
    \BI $\epsilon \to z$,
    \I  $-\omega^2 E \to B_i(x_i)$, and
    \I  $\curl \mu^{-1} \curl E \to d_i(x_i)$. \EI
In general, the physics residual will be bi-affine in $x_i$ and $z$ 
    as long as $\epsilon$ is a linear function of $z$.

In contrast, in the most general case where $\epsilon(z)$ is not linear 
    we can only say that
    \E{}{A_i(z)x_i - b_i(z) = r_i(x_i, z) = 0,}
    where $r_i(x_i, z)$ is a generalized residual function.


\subsection{Definition of the field design objective}
Although the field design objective $f_i(x_i)$ 
    can take on virtually any form,
    we choose to define it very specifically as
    \E  {field design objective definition}
        {f_i(x_i) = \sum_j I_+(|c_{ij}\T x_i| - \alpha_{ij})
            + I_+(\beta_{ij} - |c_{ij}\T x_i|),}
    where $c_{ij} \in \comps^m$ and 
    $I_+$ is the indicator function on nonnegative reals,
    \E{}{I_+(u) = \begin{cases} 0 & u \ge 0, \\ 
                                \infty & u < 0. \end{cases}}

Such a design objective implements the constraints 
    $\alpha_{ij} \le |c_{ij}\T x_i| \le \beta_{ij}$
    which can be interpreted physically as 
    constraining the power emitted into the optical modes
    represented by $c_{ij}$.

\subsection{Choice of the structure design objective}
In contrast to the narrow definition of the field design objective,
    the structure design objective is relatively unconstrained;
    taking on various forms to best suit the needs of
    the structure parameterization in use.

\section{Properties of the problem}
We now examine the general mathematical properties 
    of the stated problem \eq{rigorous problem statement}.

\subsection{Convexity analysis}
% \subsubsection{Joint non-convexity}
First we note that, as presented, 
    \eq{rigorous problem statement} is non-convex 
    in the variables $x_i$ and $z$. % Ref boyd book
Not only are the physics residuals $A_i(z)x_i - b_i(z)$ non-convex,
    but the field design objective, in that it implements the 
    $ \alpha_{ij} \le |c_{ij}\T x_i| $ constraint, is non-convex as well.

That our problem is non-convex means that it is fundamentally hard to solve
    because of the existence of multiple local minima.
Additionally, even if we were to arrive at the global maxima,
    we would not have a straightforward way to verify global optimality.
Lastly, fast convergence even to local minima may be difficult
    because methods such as Newton's method
    can not be directly applied to non-convex problems.

% \subsubsection{Separable convexity}
That said, \eq{rigorous problem statement} is \emph{separably convex} 
    in $x_i$ and $z$ for the special case where
    $\epsilon$ is linear with respect to $z$, and
    where we ignore the non-convexity
    in the field design objectives, $f_i(x_i)$, and
    assume that the structure design objective, $g(z)$, is convex as well.
This is given because of the bi-affine property
    of the physics residual, as shown in \eq{bi-affine property}.

The separably convex, or \emph{bi-convex}, properties of this special case
    motivate us to use the use of alternating direction algorithms,
    and specifically the alternating directions 
    methods of multipliers (ADMM), % Ref. 
    for the implementation of a global optimization paradigm,
    as referred to in section \ref{capabilities of our software}.
Of course, in this context we do not use the term ``global'' rigorously,
    but only to differentiate it from strategies that rely
    purely on local information.

Lastly, since the general problem is not bi-convex 
    in the case of nonlinear $\epsilon(z)$ or non-convex design objectives,
    extensions to alternating direction algorithms are necessarily employed.

% \subsection{Optimality condition}
% Based on the problem definition in \eq{rigorous problem statement},
%     we can write down a set of equations that, when met,
%     signal that we have arrived at a locally optimal point.
% Otherwise known as the Karush-Kuhn-Tucker (KKT) conditions, % Ref boyd
%     these are, for \eq{rigorous problem statement},
%     \EE {KKT conditions} % Check how these work out for complex values!!!
%         {|c_{ij}\T x_i| - \beta_{ij} &\le 0, \\
%         \alpha_{ij} - |c_{ij}\T x_i| &\le 0, \\
%         A_i(z)x_i - b_i(z) &= 0, \\
% %         % Lambda dual variables not needed because they do not appear 
% %         % in the last KKT condition.
% %         \lambda_{\alpha ij} &\ge 0, \\ 
% %         \lambda_{\beta ij} &\ge 0, \\
% %         \lambda_{\alpha ij}(|c_{ij}\T x_i| - \beta_{ij}) &= 0, \\
% %         \lambda_{\beta ij}(\alpha_{ij} - |c_{ij}\T x_i|) &= 0, \\
%         \nabla_z g(z) + \sum_i B(x_i)\T \nu_i &= 0,
%         }
%     where $\nu_i \in \comps^n$ are dual variables.


\chapter{The optimization module}

\section{Capabilities of the module}\label{capabilities of our software}
We have implemented various 
    \emph{optimization paradigms} and \emph{structure parameterizations}
    which a user can arbitrarily combine in order to 
    to solve \eq{the plan (multi-mode)}.

Specifically, the user can choose to work in either
    a \emph{local} or \emph{global} optimization paradigm:
    \BI the local paradigm uses the adjoint method
            to find small changes in the structure which
            will decrease the design objective;
    \I  the global paradigm uses the objective-first method
            to arrive at a structure by forcing
            the design objective to be met from the start. \EI

In addition, the user can choose between continuous or discrete
    structure parameterizations:
    \BI the continuous parameterization constrains the values of $z$
        to be within a continous, bounded range; on the other hand,
    \I  the discrete parameterization constrains the values of $z$ 
        within a finite set of allowable values. \EI


\section{Structure of the module}

The module is naturally separated into two submodules, 
    the optimization paradigm (or simply, paradigm) submodule 
    and the structure parameterization (or simply, structure) submodule.
The interaction between these two submodules is illustrated in \fig{strategy}.

\begin{figure}[ht]\begin{center}
\begin{pspicture}(6,5)(-6,-2)
\psset{gridcolor=green, subgridcolor=yellow}
    \let\psgrid\relax

    % Paradigm
    \rput(0,4.3){optimization paradigm}
    \psline(-2.2,4)(2.2,4) % top
    \psline(-2.2,4)(-2.2,2) % left
    \psline(2.2,4)(2.2,2) % right
    \psline(-2.2,2)(-1,2)
    \psline(2.2,2)(1,2)
    \psline(-1,2.2)(1,2.2)
    \psline(-1,2)(-1,2.2)
    \psline(1,2)(1,2.2)

    % Parameterization
    \psframe(-.8,0.8)(.8,-0.4) 
    \rput[t](0,-0.6){\parbox{3cm}{\center structure\\ parameterization}}

    % Arrows
    \rput[r](-4.3,3){$z_\text{guess}$}
    \psline{->}(-4,3)(-2.4,3)

    \rput[r](-1.9,0.8){$h(z)$}
    \psline(-1.6,1.8)(-1.6,0.2)
    \psline{->}(-1.6,0.2)(-1,0.2)

    \rput[l](1.9,0.8){$\text{argmin}\, g(z) + h(z)$}
    \psline(1.6,0.2)(1,0.2)
    \psline{<-}(1.6,1.8)(1.6,0.2)


    \rput[l](4.3,3){$z_\text{final}$}
    \psline{<-}(4,3)(2.4,3)
\end{pspicture}
\caption{Basic layout of the module.
        The optimization paradigm submodule
            accepts an initial structure $z_\text{guess}$
            and returns a final, optimized structure $z_\text{final}$.
        This is accomplished by repeatedly passing functions $h(z)$ 
            to the structure parameterization module,
            which returns an updated structure $z$
            which minimizes $g(z) + h(z)$.}
\label{fig:strategy}
\end{center} \end{figure}

In essense, a design is achieved via repeated calls to the structure submodule
    by the paradigm submodule,
    in which subsequent $h(z)$ are minimized.

\section{The paradigm submodule}
\subsection{The local optimization paradigm}
The local optimization paradigm applies the adjoint method to solve \eq{rigorous problem statement}
    in a manner analogous to the steepest-descent strategy.
Specifically, we compute the derivative of the total field design objective with respect to $z$,
    \E{total derivative of field design objective}
        {\grad_z F = \sum_i^N \grad_z f_i(x_i),}
    and then pass 
    \E{}{h(z) = \frac{1}{2}\norm{z-z_0}^2 + \kappa \grad_z F^\cT (z-z_0)}
    to the structure submodule, for some $\kappa$.

The analogy with a steepest-descent strategy is apparent since 
    the global minimum of $h(z)$ can be found via
    \E{}{\grad_z h(z) = (z - z_0) + \kappa \grad F = 0,}
    resulting in 
    \E{}{z = z_0 - \kappa \grad_z F,}
    from which we see that the role of $\kappa$ is to determine the step-size 
    in the direction of steepest-descent.

\subsubsection{Computation of $\grad_z F$}
The derivatives of the individual field design objectives are found via
    \E{}{\frac{d}{dz} f_i(x_i) = \frac{\pd f_i}{\pd x_i} \frac{dx_i}{dz}.}
Assuming that the bi-affine property of the physics residual holds,
    we can obtain ${dx_i}/{dz}$ 
    via differentiation of the corresponding physics residual, 
    $A_i(z) x_i - b_i(z)$,
    \E{}{A_i(z) dx_i + dA_i(z) - db_i(z) = A_i(z)dx_i + B_i(x_i)dz,}
    where $dA_i(z) - db_i(z) = B_i(x_i)dz$.

Assuming that physics is already satisfied, $A_i(z) x_i - b_i(z) = 0$,
    and that we want to keep the physics residual at 0,
    the following condition must then be satisfied,
    \E{}{A_i(z) dx_i = -B_i(x_i) dz}
    from which we obtain
    \E{}{\frac{dx_i}{dz} = -A_i(z)^{-1} B_i(x_i).}

Efficient calculation of $dx_i/dz$ is almost always impossible since $B_i(x_i) \in \comps^{n \times n}$.
At the same time, since ${\pd f_i}/{\pd x_i} \in \comps^{1 \times m}$, we instead compute $df_i(x_i)/dz$ via
    \E{}{\frac{d}{dz} f_i(x_i) = -\frac{\pd f_i}{\pd x_i} A_i(z)^{-1} B_i(x_i) = 
        -\left(A_i(z)^{-\cT} \frac{\pd f_i}{\pd x_i}^\cT\right)^\cT B_i(x_i)}
    or
    \E{adjoint grad_z f}
        {\grad_z f_i =  - B_i(x_i)\T A_i(z)^{-\cT} \grad_{x_i} f_i}
    which requires only one solve of $A_i^\cT$ as opposed to $n$ solves of $A_i$.

For the case where $\epsilon$ is a nonlinear function of $z$,
    we can simply replace $B_i(x_i)$ with 
    $\tilde{B_i}(x_i, z) = \grad_z r_i(x_i, z)$.

\subsubsection{Computation of $\pd f_i(x_i) / \pd x_i$}
The definition of $f_i(x_i)$ as given in \eq{field design objective definition} is not differentiable 
    since any deviation away from the power constraints results in $f_i = \infty$.
In order to make the constraints differentiable, then, 
    we use the following relaxed indicator function $I^\text{rel}_+$ in place of $I_+$,
    \E{}{I^\text{rel}_+(u) = \begin{cases} 0 & u \ge 0, \\ 
                                \frac{1}{a}|u|^p & u < 0, \end{cases}}
    where $a$ is a normalization factor and $p \in (0, \infty]$. 
In order to guarantee a well-defined value of $\pd f_i(x_i) / \pd x_i$ even for $p = \infty$,
    we can let $a = \max_i f_i(x_i)$.



\subsection{The global optimization paradigm}
In contrast to the local paradigm, 
    the global optimization paradigm utilizes an objective-first strategy
    in the design process.
Although such a strategy is not technically global,
    it is differentiated from local strategies because
    it strictly enforces all design objectives,
    even at the expense of non-zero physics residuals.

The optimization paradigm utilizes 
    the alternating directions method of multipliers\cite{ADMM} (ADMM) optimization technique, 
    which casts \eq{rigorous problem statement} in terms of the following augmented Lagrangians, 
    \E{}{\Lag(x, z, y) = \sum_i^N f_i(x_i) 
            + \frac{\rho}{2} \norm{A_i(z) x_i - b_i(z)}^2 
            +\real\left[y_i^\cT (A_i(z)x_i-b_i(z))\right] + g(z),}
    where $\rho$ is a constant scalar and 
    $y_i$ are dual variables. If we introduce $u_i = y_i / \rho$, then 
    \E{}{\Lag(x,z,u) = \sum_i^N f_i(x_i)
            + \frac{\rho}{2} \norm{A_i(z) x_i - b_i(z) + u_i}^2 
            - \frac{\rho}{2} \norm{u_i}^2 + g(z).}

ADMM obtains the solutions for $x$ and $z$ via
    \EE{}{  x_i &\gets \argmin_{x_i} \Lag(x, z, u), \quad i = 1,\ldots,N \\
            z &\gets \argmin_{z} \Lag(x, z, u), \\
            u_i &\gets u_i + (A_i(x_i) z - b(x_i)), \quad i = 1,\ldots,N.}

To obtain the correct $h(z)$ which should be passed to the structure submodule,
    we use \eq{bi-affine property} to write
    \E{}{\argmin_z \Lag = \argmin_z g(z) 
            + \sum_i^N \frac{\rho}{2} \norm{B_i(x_i) z - d(x_i) + u_i}^2}
    which means that
    \E{}{h(z) = \sum_i^N \frac{\rho}{2} \norm{B_i(x_i) z - d(x_i) + u_i}^2,}
    or more generally,
    \E{}{h(z) = \sum_i^N \frac{\rho}{2} \norm{r_i(x_i, z) + u_i}^2.}


\subsubsection{Computation of $\argmin_{x_i} \Lag$}
Updating $x_i$, which is left to the paradigm submodule,
    involves solving $\argmin_{x_i} \Lag(x, z, u)$ via Newton's method for all $i = 1, \ldots, N$.
This can be accomplished via the gradient with respect to $x_i$
    \E{}{\grad_{x_i}\Lag(x, z, u) = \grad_{x_i} f_i(x_i)
                + \rho A_i(z)\T (A_i(z) x_i - b_i(z) + u_i),}
    as well as the Hessian with respect to $x_i$
    \E{}{\grad^2_{x_i}\Lag(x, z, u) = \grad^2_{x_i} f_{x_i} + \rho A_i(z)\T A_i(z),}
    which can be combined to determine the direction of a step in Newton's method,
    \E{}{\Delta x_i = - \grad^2_{x_i} \Lag^{-1} \grad_{x_i} \Lag.}

Solving $\argmin_{x_i} \Lag(x, z, u)$ from an initial $x_i$ thus proceeds by
    repeatedly computing $\Delta x_i$ and then performing a line search along that direction,
    until some convergence criterion is met, typically
    \E{}{\frac{1}{2} \grad_{x_i}\Lag\T \grad^2_{x_i} \Lag^{-1} \grad_{x_i} \Lag \le \text{error threshold}.}

\subsubsection{Relaxation of $f_i(x_i)$}
In order to compute $\Delta x_i$ we relax the design objectives, $f_i(x_i)$, to a differentiable, and convex form.
Specifically, the upper bound constraint is relaxed via
    \E{}{I_+(\beta_{ij} - |c\T_{ij} x_i|) \to -\frac{1}{t} \ln (\beta_{ij}^2 - \norm{c\T_{ij} x_i}^2),}
    where $t$ is a real positive scalar. The lower bound constraint is relaxed via
    \E{}{I_+(|c\T_{ij} x_i| - \alpha_{ij}) \to -\frac{1}{t} \ln (\real[e^{-i\phi_{ij}}c\T_{ij} x_i] - \alpha_{ij}),}
     where $e^{-i\phi_{ij}}$ is a phase factor on $c\T_{ij} x_i$.
The convention of $\phi_{ij} = \text{angle}(c\T_{ij} x_i)$ is often used
    to determine $\phi_{ij}$ before optimizing for $x_i$.

The gradient can now be obtained as
    \E{}{\grad_{x_i} f_i(x_i) = \sum_j \frac{r_{ij}}{t} c_{ij}}
    where
    \E{}{r_{ij} = \frac{2c\T_{ij} x_i}{\beta_{ij}^2 - \norm{c\T_{ij} x_i}^2} 
                - \frac{e^{i\phi_{ij}}}{\real[e^{-i\phi_{ij}} c\T_{ij} x_i] - \alpha_{ij}},}
    and the Hessian as 
    \E{}{\grad_{x_i}^2 f_i(x_i) = \sum_j \frac{s_{ij}}{t} c_{ij} c\T_{ij}}
    where
    \E{}{s_{ij} = \frac{2}{\beta_{ij}^2 - \norm{c\T_{ij} x_i}^2}
                + \frac{4\norm{c\T_{ij} x_i}^2}{(\beta_{ij}^2 - \norm{c\T_{ij} x_i}^2)^2}
                + \frac{1}{(\real[e^{-i\phi_{ij}} c\T_{ij} x_i] - \alpha_{ij})^2}.}



\subsubsection{Computation of $\Delta x_i$}
We now show how $\Delta x_i$ may be efficiently computed.
First we introduce
    \E{}{\tilde{c}_i = \frac{1}{\rho t} \sum_j r_j A^{-\cT} c_{ij}}
    in order to write
    \E{}{\grad_{x_i} \Lag(x, z, u) = \rho A_i(z)\T(A_i(z)x_i - b_i(z) 
                            + u_i + \tilde{c}_i).}
For its part, the Hessian can be simplified using the matrix inversion lemma\footnote{$(A + UCV)^{-1} = A^{-1} - A^{-1}U (C^{-1} + V A^{-1} U)^{-1} V A^{-1}$}.
Coupled with the introduction of 
    \E{}{\tilde{C}_i = A_i(z)^{-\cT}\begin{bmatrix} 
                        \sqrt{\frac{s_1}{\rho t}}c_{i1} &
                        \sqrt{\frac{s_2}{\rho t}}c_{i1} &
                        \cdots &
                        \sqrt{\frac{s_{m_i}}{\rho t}}c_{im_i} 
                        \end{bmatrix}}
    we can write
    \E{}{\grad_{x_i}^2 \Lag(x, z, u) = \rho A_i(z)\T (I + \tilde{C}\tilde{C}\T) A_i(z),}
    and more importantly
    \E{}{(\grad_{x_i}^2 \Lag(x, z, u))^{-1} = \frac{1}{\rho} A_i(z)^{-1} M A_i(z)^{-\cT},}
    where
    \E{}{M = (I + \tilde{C} \tilde{C}\T)^{-1} = 
            I - \tilde{C} (I + \tilde{C}\T\tilde{C})^{-1} \tilde{C}\T.}
 
Finally, the expression for $\Delta x_i$ is obtained as
    \E{}{\Delta x_i = - \grad^2_{x_i} \Lag^{-1} \grad_{x_i} \Lag = 
        A_i(z)^{-1} M (A_i(z)x_i - b_i(z) + u_i + \tilde{c}_i).}
        
    
\section{The structure submodule}


\begin{thebibliography}{99}
\bibitem{ADMM} Boyd group ADMM paper
\end{thebibliography}
\end{document}
